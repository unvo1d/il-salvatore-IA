{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjnc58pi93ps"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3iSYTXs93pu"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj14eGNt93pv"
      },
      "source": [
        "## Natural-language processing: The bird's eye view\n",
        "\n",
        "### Preparing text data:\n",
        "\n",
        "1. Text standardization\n",
        "2. Text splitting (tokenization)\n",
        "3. Vocabulary indexing\n",
        "\n",
        "All this can be done with explicit functions in python, but as with many other tasks, it is much quicker (and easier) using the TextVectorization layer in keras (later in the notebook).\n",
        "\n",
        "The first part is also intented to understand the preprocessing of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaPUlwR-93pw"
      },
      "source": [
        "# 1. Using explicit functions in python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8hJG-LB93pw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YztuTXF793px"
      },
      "outputs": [],
      "source": [
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JoJODyg93py",
        "outputId": "2338eee8-29d5-4dc3-b54c-7486c7509f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'': 0, '[UNK]': 1, 'i': 2, 'write': 3, 'erase': 4, 'rewrite': 5, 'again': 6, 'and': 7, 'then': 8, 'a': 9, 'poppy': 10, 'blooms': 11}\n"
          ]
        }
      ],
      "source": [
        "print(vectorizer.vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPhbqbZW93pz",
        "outputId": "b9466e8b-7f93-4e19-c997-e038c638f801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '', 1: '[UNK]', 2: 'i', 3: 'write', 4: 'erase', 5: 'rewrite', 6: 'again', 7: 'and', 8: 'then', 9: 'a', 10: 'poppy', 11: 'blooms'}\n"
          ]
        }
      ],
      "source": [
        "print(vectorizer.inverse_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jiUW--793pz",
        "outputId": "64c933d4-02df-4ac2-a80f-02f3c1b32ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzo2Pz6s93p0",
        "outputId": "1ebcb27e-d1e2-4a7f-e471-c829010c4bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g98GOH8693p1"
      },
      "source": [
        "# 2. Using keras tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqlJp4O293p1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI3Kczg293p2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VbLaL9x93p3"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qcFrvHu93p3"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pimn5l7q93p3",
        "outputId": "c11f7c62-2151-4c61-9d39-ab701fb50525"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0lNDe_93p4",
        "outputId": "c1d76fce-deec-4d9e-c016-1b2d7a90aa8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blykbnv493p5",
        "outputId": "3cf152de-7a65-4c23-9e42-024988522c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhXmJihK93p5"
      },
      "source": [
        "# 3. Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih5M2zgm93p5"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5KpXdni93p5",
        "outputId": "3f10c91e-c530-48d8-b2a8-79c4701f1dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  44.5M      0  0:00:01  0:00:01 --:--:-- 44.4M\n"
          ]
        }
      ],
      "source": [
        "# Descarga el dataset completo IMDB\n",
        "\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZv3lX1g93p6"
      },
      "outputs": [],
      "source": [
        "# Descomprime el .tar.gz descargado\n",
        "# OJO SOLO FUNCIONA EN LINUX - Windows: hacer el untar desde sistema operativo\n",
        "\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x83V_MeV93p6"
      },
      "outputs": [],
      "source": [
        "# Borra un directorio que no se va a utilizar\n",
        "\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8HHkunF93p6",
        "outputId": "7bab6a8c-7f9d-4037-f197-9ca8eeabbd90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat ./aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjeoJ3kI93p7"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXsZlVKM93p7",
        "outputId": "36e2b025-b399-47fe-ab5e-5e881f869709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhxxB7Oo93p7"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ6rOj_593p8",
        "outputId": "f1a11000-4686-4443-dc30-6865d0f8b652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"Most people, when they think of expressionist cinema, look to the b&w German films of the silent and early sound eras--films that emphasized canted angles, extreme contrasts of light and dark, exaggerated performance, and occasional uses of surrealism to create a dreamlike atmosphere in order to diverge from traditional, naturalistic modes of cinematic representation. If we're willing to accept that the Germans were not the only filmmakers to create expressionist cinema (and that those above-mentioned characteristics are not prerequisites for expressionist film), then I would argue that Dodes'ka-den (DKD) is a prime example of this type of film. <br /><br />Like Dreams, DKD is a little unhinged for a Kurosawa film, dabbling, as it does, in the unreal. However, DKD is also, unlike Dreams, a great film and probably my favorite Kurosawa picture. Why? Mostly, I think, it's the colors. This was, I believe, Kurosawa's first color film, and the man saturates the movie with vibrant primary colors, creating a completely unreal contemporary Japan. We are used to the neon lights and gleaming Tokyo skyscrapers; we are not used to a city that appears to have been colored with crayons. <br /><br />DKD is, as I said, a peculiar film inasmuch as many of its characters live in a junkyard, appearing to live in an alternate universe. That is, I think, the point--these are the Tokyo outsiders, the people left behind during the great move forward following World War II. The film also represents one of Kurosawa's more heartfelt movies; there is genuine sentiment here and genuine pathos (such as when the boy's father describes their dream home). It's an amazingly moving film from a man better known for stunning, John Ford-like vistas and samurais. Everyone should have known Kurosawa had in him a movie as touching and thought provoking as this (Ikiru foreshadows the emotional resonance of this film in many ways). <br /><br />I will also argue, to the last, that this is Kurosawa's greatest achievement. His samurai films, though capable pictures, pale in comparison to works by Kobayashi (Hara-kiri is the greatest, most intelligent samurai film committed to celluloid). Rashomon, Hidden Fortress, Seven Samurai, Yojimbo, Sanjuro, Kagemusha, and Ran are all fine films, but they're merely good (and, frankly, I think that word is too generous for Hidden Fortress and Kagemusha). DKD is a great movie, as is Ikiru. They are the crown jewels that show Akira was not a one-trick samurai pony. They reveal his artistry and mastery of cinema.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi8UB1ZW93p8"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbuh7jp293p8"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0c3CMLD93p8"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrfCwPpm93p8"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE7t0h-B93p9"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvNqUUVy93p9",
        "outputId": "02045460-7e9c-411b-ac2c-4da8db0219ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyOnOBnb93p9"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VDy2Prp93p9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, \n",
        "              hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O-PUfOi93p-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JonKoyiD93p-"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjRRgBcI93p-",
        "outputId": "c358a131-a4b5-4e6e-8203-c9f08bc4ec88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui2pT6JB93p-"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CD_2Ph793p_",
        "outputId": "786eb2fa-f0d8-47d0-e5b2-d27038f2c753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 17s 25ms/step - loss: 0.4145 - accuracy: 0.8270 - val_loss: 0.2870 - val_accuracy: 0.8860\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2741 - accuracy: 0.9000 - val_loss: 0.2772 - val_accuracy: 0.8926\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2414 - accuracy: 0.9151 - val_loss: 0.2788 - val_accuracy: 0.8960\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2248 - accuracy: 0.9253 - val_loss: 0.2895 - val_accuracy: 0.8982\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2182 - accuracy: 0.9300 - val_loss: 0.3002 - val_accuracy: 0.8978\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2137 - accuracy: 0.9316 - val_loss: 0.3191 - val_accuracy: 0.8904\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2105 - accuracy: 0.9337 - val_loss: 0.3206 - val_accuracy: 0.8918\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2011 - accuracy: 0.9349 - val_loss: 0.3217 - val_accuracy: 0.8926\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 7s 10ms/step - loss: 0.2003 - accuracy: 0.9402 - val_loss: 0.3295 - val_accuracy: 0.8914\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1958 - accuracy: 0.9399 - val_loss: 0.3382 - val_accuracy: 0.8920\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe35abc00d0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVBiu1O693p_",
        "outputId": "aa7a2876-c149-4000-f1ac-a0e8784cf950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 11s 14ms/step - loss: 0.2940 - accuracy: 0.8873\n",
            "Test acc: 0.887\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj_7Q6UR93p_"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IwWa0rR93p_"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19pLsxmp93qA"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDRkabtl93qA"
      },
      "source": [
        "## what does Text Vectorization do\n",
        "\n",
        "From\n",
        "https://keras.io/getting_started/intro_to_keras_for_engineers/\n",
        "\n",
        "https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olcc2l3n93qA",
        "outputId": "fd363776-0412-4c02-bc00-e390bddcbe3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            " [0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.]], shape=(2, 17), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Example training data, of dtype `string`.\n",
        "example_data = np.array([[\"This is the 1st sample.\"], \n",
        "                         [\"And here's the 2nd sample.\"]])\n",
        "\n",
        "# Create a TextVectorization layer instance. It can be configured to either\n",
        "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
        "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
        "# configurable.\n",
        "example_vectorizer = TextVectorization(output_mode=\"multi_hot\", \n",
        "                                       ngrams=2)\n",
        "\n",
        "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
        "# index for the data, which can then be reused when seeing new data.\n",
        "example_vectorizer.adapt(example_data)\n",
        "\n",
        "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
        "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
        "# token.\n",
        "integer_data = example_vectorizer(example_data)\n",
        "print(integer_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIrejNq93qA"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4aK0RdD93qB"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdXA1Oey93qB",
        "outputId": "b659d5b2-b237-4d16-b5c2-c70f356025a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xSLhEEh93qB"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9xxnJSH93qB",
        "outputId": "c47d26d0-15c1-44a2-b203-46f607ce7900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 16s 24ms/step - loss: 0.3772 - accuracy: 0.8485 - val_loss: 0.2636 - val_accuracy: 0.8946\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2429 - accuracy: 0.9135 - val_loss: 0.2655 - val_accuracy: 0.9000\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2088 - accuracy: 0.9319 - val_loss: 0.2789 - val_accuracy: 0.8966\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1883 - accuracy: 0.9421 - val_loss: 0.2961 - val_accuracy: 0.8984\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1840 - accuracy: 0.9466 - val_loss: 0.3104 - val_accuracy: 0.9034\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1818 - accuracy: 0.9471 - val_loss: 0.3165 - val_accuracy: 0.9008\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1839 - accuracy: 0.9509 - val_loss: 0.3387 - val_accuracy: 0.8968\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1756 - accuracy: 0.9523 - val_loss: 0.3378 - val_accuracy: 0.8998\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1690 - accuracy: 0.9514 - val_loss: 0.3373 - val_accuracy: 0.9004\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1681 - accuracy: 0.9528 - val_loss: 0.3472 - val_accuracy: 0.8974\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe35dbce190>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HPuMYWa93qB",
        "outputId": "fa6987dc-c27f-43c8-a660-4c3cf71690b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 11s 14ms/step - loss: 0.2646 - accuracy: 0.8992\n",
            "Test acc: 0.899\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8nUkkfy93qC"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrH_ayaj93qC"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GxixeqO93qC"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLt3WDP93qC"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al29Mir693qD"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRIsN3J693qD"
      },
      "source": [
        "## Training the TF-IDF bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSCRBNKb93qD"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40cCPK-i93qE",
        "outputId": "2686b9f2-e2b7-4a1b-9c39-ebcb6b6161a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqzfVIay93qE"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWWIhXRj93qE",
        "outputId": "f4f5f167-8315-41c4-b906-692d1d77d61b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.4943 - accuracy: 0.7843 - val_loss: 0.2987 - val_accuracy: 0.8808\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.3382 - accuracy: 0.8651 - val_loss: 0.2962 - val_accuracy: 0.8770\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.3013 - accuracy: 0.8769 - val_loss: 0.3074 - val_accuracy: 0.8802\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2765 - accuracy: 0.8834 - val_loss: 0.3303 - val_accuracy: 0.8528\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2647 - accuracy: 0.8903 - val_loss: 0.3130 - val_accuracy: 0.8754\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2619 - accuracy: 0.8870 - val_loss: 0.3225 - val_accuracy: 0.8634\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2482 - accuracy: 0.8940 - val_loss: 0.3333 - val_accuracy: 0.8620\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2504 - accuracy: 0.8907 - val_loss: 0.3332 - val_accuracy: 0.8658\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2413 - accuracy: 0.8946 - val_loss: 0.3409 - val_accuracy: 0.8732\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2417 - accuracy: 0.8971 - val_loss: 0.3420 - val_accuracy: 0.8646\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe35d9aaeb0>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1rmGTv5893qE",
        "outputId": "5a54576d-4e78-4ba3-ecdc-1dc72dedc91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 11s 14ms/step - loss: 0.3127 - accuracy: 0.8645\n",
            "Test acc: 0.865\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luZewAUs93qF"
      },
      "source": [
        "## Testing the TF-IDF bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SdeJ72DK93qF"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), \n",
        "                     dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7raAYzNE93qF",
        "outputId": "250dad1a-4808-47d4-ad9f-19225a28ea2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96.47 percent positive\n"
          ]
        }
      ],
      "source": [
        "#import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UsY8-efl93qF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}