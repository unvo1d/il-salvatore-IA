{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200%; font-weight: bold; color: maroon;\">401_Intro_to_numpy</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some introduction to numpy\n",
    "\n",
    "numpy (most frequently imported as np) is the linear algebra library for python environments. \n",
    "\n",
    "In order to work -to implement vectorization, which is the basis of the computational advantage of numpy- you have first to define a numpy object: a matrix.\n",
    "\n",
    "Once defined a matrix, ie x, you can simply call vectorized functions using np. syntax.\n",
    "\n",
    "For instance:\n",
    "\n",
    "- np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
    "\n",
    "In summary, numpy has efficient built-in functions for computing matrices, it is fast because it vectorizes the computations\n",
    "\n",
    "\n",
    "The following is adapted from the great Andrew Ng's coursera on deep learning\n",
    "\n",
    "## Important numpy links / reference material \n",
    "\n",
    "numpy official user guide (good, but long):  https://numpy.org/doc/stable/numpy-user.pdf\n",
    "\n",
    "one simple / quick numpy cheatsheet : https://s3.amazonaws.com/dq-blog-files/numpy-cheat-sheet.pdf\n",
    "\n",
    "yeat another (longer) cheatsheet: http://datacamp-community-prod.s3.amazonaws.com/ba1fe95a-8b70-4d2f-95b0-bc954e9071b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In machine or deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "x1 = [random.random() for e in range(10**4)]  # remember list comprehension\n",
    "x2 = [random.random() for e in range(10**4)]\n",
    "\n",
    "#print(x1)\n",
    "#print(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz1 = np.array([[1., 0., 0.],\n",
    "                    [0., 1., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]]], dtype=int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unos = np.ones((2, 3, 4), dtype=np.int16)\n",
    "unos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleat1 = np.random.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.78978082, 0.18506534, 0.86595195, 0.68792891],\n",
       "        [0.49610107, 0.18975494, 0.79358076, 0.88819531],\n",
       "        [0.42436845, 0.21204149, 0.01761702, 0.08787394]],\n",
       "\n",
       "       [[0.43770643, 0.86918783, 0.01427644, 0.29007259],\n",
       "        [0.29645345, 0.03444438, 0.76924148, 0.85706888],\n",
       "        [0.36827647, 0.2101472 , 0.65802431, 0.94326462]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aleat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2120414898341021"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aleat1[0, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [5, 3], [4, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [5, 3],\n",
       "       [4, 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x1))\n",
    "n1 = np.array(x1)\n",
    "n1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next example code, we will make basic matrix computations **without** numpy, this is using classical for loops. \n",
    "\n",
    "Later we will do the same but using vectorization, ie, numpy\n",
    "\n",
    "NOTE:\n",
    "\n",
    "- np.zeros(x, y) produces a matrix with zeros with dimensions x, y\n",
    "\n",
    "- np.random.rand(x, y) creates a random float numbers matrix with dimensions x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 2540.6640281171935\n",
      "\n",
      " ----- Computation time dot product (for) = 2.7200519999999173ms\n",
      "\n",
      " ----- Computation time outer prod (for)= 41960.232128999996ms\n",
      "\n",
      " ----- Computation time elementwise (for)= 4.2035289999944325ms\n",
      "\n",
      " ----- Computation time dot prod general (for) = 20.257917000002124ms\n"
     ]
    }
   ],
   "source": [
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot))\n",
    "print (\"\\n ----- Computation time dot product (for) = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "#print (\"outer = \" + str(outer))\n",
    "print (\"\\n ----- Computation time outer prod (for)= \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "#print (\"elementwise multiplication = \" + str(mul))\n",
    "print (\"\\n ----- Computation time elementwise (for)= \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "#print (\"gdot = \" + str(gdot))\n",
    "print (\"\\n ----- Computation time dot prod general (for) = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Look for the np methods for:\n",
    "\n",
    "1. dot product of vectors\n",
    "\n",
    "2. outer product of vectors\n",
    "\n",
    "3. elementwise multiplication\n",
    "\n",
    "4. general dot product of W (previously generated) and x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotprod = np.dot(x1, x2)\n",
    "print(dotprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array(x1)\n",
    "vector1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----- Computation time dot prod (np) = 4.781159000003754ms\n",
      "\n",
      " ----- Computation time outer (np) = 1123.3817110000005ms\n",
      "\n",
      " ----- Computation time elementwise (np) = 2.078986000000782ms\n",
      "\n",
      " ----- Computation time dot prod general (np) = 1.3426630000026307ms\n"
     ]
    }
   ],
   "source": [
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "prodscalar = np.dot(x1, x2)\n",
    "toc = time.process_time()\n",
    "print (\"\\n ----- Computation time dot prod (np) = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outernp = np.outer(x1, x2)\n",
    "toc = time.process_time()\n",
    "print (\"\\n ----- Computation time outer (np) = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "elementwise = np.multiply(x1, x2)\n",
    "toc = time.process_time()\n",
    "print (\"\\n ----- Computation time elementwise (np) = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "generaldot = np.dot(W, x1)\n",
    "toc = time.process_time()\n",
    "print (\"\\n ----- Computation time dot prod general (np) = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generaldot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, you simply **cannot compute them without numpy**. \n",
    "\n",
    "**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to  `.*` in Matlab/Octave), which performs an element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and cost functions for binary target \n",
    "## i.e. as logistic regression, or those linking x and y through sigmoid function\n",
    "\n",
    "**Reminder**:\n",
    "\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). \n",
    "- In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- For binary targets loss is defined as:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR FUNCTION\n",
    "\n",
    "def loss(a, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat = a =  vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the loss function defined above **for each target element**\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = -y * np.log(a) - (1-y) * (np.log(1-a))               # compute cost    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = [0.10536052 0.22314355 0.10536052 0.91629073 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"loss = \" + str(loss(a,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **loss** </td> \n",
    "       <td> [0.10536052 0.22314355 0.10536052 0.91629073 0.10536052] </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise you must solve\n",
    "\n",
    "## Write the cost function\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Once you have computed the loss function you can easily compute (using numpy) the cost function (which is not more than the average of the loss functions across the y or actual target vector).\n",
    "\n",
    "The cost can be computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: compute cost\n",
    "\n",
    "def cost(a, y):\n",
    "    \"\"\"\n",
    "    Computes the cost function by summing loss over all training examples.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- A numpy vector or array\n",
    "    y -- A scalar or numpy vector\n",
    "    \n",
    "    You must obtain\n",
    "    m -- the size of y (use len(y))\n",
    "    \n",
    "    Return:\n",
    "    cost -- Your computed cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    ### HINT : Define first m as a property of the input object y\n",
    "    m = len(y)\n",
    "    cost = 1/m * np.sum(loss(a, y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = [0.10536052 0.22314355 0.10536052 0.91629073 0.10536052]\n",
      "cost = 0.29110316603236874\n"
     ]
    }
   ],
   "source": [
    "a = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"loss = \" + str(loss(a,y)))\n",
    "print(\"cost = \" + str(cost(a,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: compute cost including the error in the same line of code\n",
    "\n",
    "def cost(a, y):\n",
    "    \"\"\"\n",
    "    Computes the cost function by summing loss over all training examples.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- A numpy vector or array\n",
    "    y -- A scalar or numpy vector\n",
    "    \n",
    "    You must obtain\n",
    "    m -- the size of y (use len(y))\n",
    "    \n",
    "    Return:\n",
    "    cost -- Your computed cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"cost = \" + str(cost(a, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to remember\n",
    "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
    "- You have reviewed the loss and cost functions for binary targets\n",
    "- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
