{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200%; font-weight: bold; color: maroon;\">602-Naive Bayes classification algorithm for detecting spam messages. \n",
    "    \n",
    "Scikit-learn + nltk</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying spam versus not-spam (called ham) is one of the best know applications of Naive Bayes classification algorithm.\n",
    "\n",
    "In this notebook you will use nltk (natural language toolkit in python) and naive bayes classification algorithm to do a *very simple Spam/Ham* Classification from SMS dataset from UCI.\n",
    "\n",
    "Notebook is taken from:\n",
    "\n",
    "https://www.kaggle.com/astandrik/simple-spam-filter-using-naive-bayes\n",
    "\n",
    "Dataset is taken from here:\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "\n",
    "And a good source of tech details (among many excellent other sources):\n",
    "https://towardsdatascience.com/how-to-build-and-apply-naive-bayes-classification-for-spam-filtering-2b8d3308501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4be8fca1-a4f6-4f4a-88ff-1539b19cafca",
    "_uuid": "eb55af3fcca73ac0a07e2ecd8709e36bcc17fc78"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 1: DATA PREPROCESSING**\n",
    "\n",
    "since the dataset comes with additional unnamed, column, I need to drop them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7611c519-0306-49c9-b042-307a3562619a",
    "_uuid": "b9701c4143d7d63f252cab0d7725222b58ef6069",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "messages.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\n",
    "messages = messages.rename(columns={'v1': 'class','v2': 'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be509646-dbc5-469d-9ae0-43dfd1cab0e2",
    "_uuid": "cd658d30fb6813499893b51455bf449635ec6a43"
   },
   "outputs": [],
   "source": [
    "messages.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[\"text\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[\"text\"][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5c541708-35c5-4460-a351-46032fc219e8",
    "_uuid": "125b4774c58068557499363295e50e7d79ce9dc5"
   },
   "outputs": [],
   "source": [
    "messages.groupby('class').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above information, we know that:\n",
    "1. only about 15% of the text messages is classified as a spam\n",
    "2. there are some duplicate messages, since the number of unique values lower than the count values of the text\n",
    "\n",
    "in the next part, lext check the length of each text messages to see whether it is correlated with the text classified as a spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d86cb88-4ea5-497a-96af-245984946265",
    "_uuid": "cbd68ec903cf7c437c885b69993cab8dec61503c"
   },
   "outputs": [],
   "source": [
    "messages['length'] = messages['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4da1b849-61f1-4963-bd36-1c306534a0e0",
    "_uuid": "961442d7f56de1058a0fd825683939c618030ed8"
   },
   "outputs": [],
   "source": [
    "messages.hist(column='length',by='class',bins=50, figsize=(15,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above figure, we can see that most of ham (or not spam) messages only have length under 200 (100 to be exact) while spam messages tend to have higher lentgh above 130 or 140 approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f3b73e5-2bc6-4a3f-8e63-b78b82ccf3d0",
    "_uuid": "467cc69f5be1d9758b549c4af5265d981e6d6dca"
   },
   "source": [
    "**PART 2: CREATE TOKENIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d10b73f4-e10c-422b-ad4b-74c6037675a4",
    "_uuid": "2070cd8a789e028dd3417798dd61a6454d990ea5"
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    '''\n",
    "    What will be covered:\n",
    "    1. Remove punctuation\n",
    "    2. Remove stopwords\n",
    "    3. Return list of clean text words\n",
    "    '''\n",
    "    \n",
    "    #1\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    #2\n",
    "    clean_words = [word for word in nopunc.split() \n",
    "                   if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    #3\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check what above code will produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb3d6f64-3e17-4baa-ab5e-730044d7b8c6",
    "_uuid": "d80940a32545d93ec1e36fabcc46a174ace0ace9"
   },
   "outputs": [],
   "source": [
    "messages['text'].apply(process_text).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 3: SPLITTING DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train, msg_test, class_train, class_test = train_test_split(messages['text'],\n",
    "                                                                messages['class'],\n",
    "                                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 4: DATA PREPROCESSING**\n",
    "\n",
    "wait, we've just created the tokenizer isn't it? let the pipeline do the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23bb61f8-1ddd-4091-bda3-48b036d93ffe",
    "_uuid": "d477922c1856520ed1a7e3cb2e38b6690af4d080"
   },
   "source": [
    "**PART 5: MODEL CREATION**\n",
    "\n",
    "here I'll just use pipeline in order to minimize effort on doing preprocessing, transforming then training data on both training dataset and test dataset. Using pipeline will handle them all in a few lines of codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=process_text)), # converts strings to integer counts\n",
    "    ('tfidf',TfidfTransformer()), # converts integer counts to weighted TF-IDF scores\n",
    "    ('classifier',MultinomialNB()) # train on TF-IDF vectors with Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 6: TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(msg_train,class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(class_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(class_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "sns.heatmap(confusion_matrix(class_test,predictions),\n",
    "            annot=True, \n",
    "            fmt=\".0f\", \n",
    "            linewidths=.5, \n",
    "            square = True, \n",
    "            cmap = 'Reds_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* we got fairly high but not good enough prediction result here, maybe if the dataset gets higher, maybe naive bayes will do its work better\n",
    "\n",
    "thanks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
