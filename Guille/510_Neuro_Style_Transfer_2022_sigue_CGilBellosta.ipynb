{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 510_Neuro_Style_Transfer_2022\n",
    "\n",
    "Este notebook se llama así porque replica otros notebooks previos (para versiones previas de Tensorflow), siempre basados en el trabajo de Chollet, específicamente en su capítulo 8 del libro [Deep Learning with Python].\n",
    "\n",
    "Una diferencia importante es que se basa en trabajo reciente de Carlos J. Gil Bellosta (https://www.datanalytics.com/):\n",
    "\n",
    "https://github.com/cjgb/style-transfer-beyond-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer\n",
    "\n",
    "\n",
    "This notebook contains the code samples found in Chapter 8, Section 3 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Besides Deep Dream, another major development in deep learning-driven image modification that happened in the summer of 2015 is neural \n",
    "style transfer, introduced by Leon Gatys et al. The neural style transfer algorithm has undergone many refinements and spawned many \n",
    "variations since its original introduction, including a viral smartphone app, called Prisma. For simplicity, this section focuses on the \n",
    "formulation described in the original paper.\n",
    "\n",
    "Neural style transfer consists in applying the \"style\" of a reference image to a target image, while conserving the \"content\" of the target \n",
    "image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![style transfer](https://s3.amazonaws.com/book.keras.io/img/ch8/style_transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is meant by \"style\" is essentially textures, colors, and visual patterns in the image, at various spatial scales, while the \"content\" \n",
    "is the higher-level macrostructure of the image. For instance, blue-and-yellow circular brush strokes are considered to be the \"style\" in \n",
    "the above example using Starry Night by Van Gogh, while the buildings in the Tuebingen photograph are considered to be the \"content\".\n",
    "\n",
    "The idea of style transfer, tightly related to that of texture generation, has had a long history in the image processing community prior \n",
    "to the development of neural style transfer in 2015. However, as it turned out, the deep learning-based implementations of style transfer \n",
    "offered results unparalleled by what could be previously achieved with classical computer vision techniques, and triggered an amazing \n",
    "renaissance in creative applications of computer vision.\n",
    "\n",
    "The key notion behind implementing style transfer is same idea that is central to all deep learning algorithms: we define a loss function \n",
    "to specify what we want to achieve, and we minimize this loss. We know what we want to achieve: conserve the \"content\" of the original image, \n",
    "while adopting the \"style\" of the reference image. If we were able to mathematically define content and style, then an appropriate loss \n",
    "function to minimize would be the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Where `distance` is a norm function such as the L2 norm, `content` is a function that takes an image and computes a representation of its \n",
    "\"content\", and `style` is a function that takes an image and computes a representation of its \"style\".\n",
    "\n",
    "Minimizing this loss would cause `style(generated_image)` to be close to `style(reference_image)`, while `content(generated_image)` would \n",
    "be close to `content(generated_image)`, thus achieving style transfer as we defined it.\n",
    "\n",
    "A fundamental observation made by Gatys et al is that deep convolutional neural networks offer precisely a way to mathematically defined \n",
    "the `style` and `content` functions. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The content loss\n",
    "\n",
    "\n",
    "As you already know, activations from earlier layers in a network contain _local_ information about the image, while activations from \n",
    "higher layers contain increasingly _global_ and _abstract_ information. Formulated in a different way, the activations of the different \n",
    "layers of a convnet provide a decomposition of the contents of an image over different spatial scales. Therefore we expect the \"content\" of \n",
    "an image, which is more global and more abstract, to be captured by the representations of a top layer of a convnet.\n",
    "\n",
    "A good candidate for a content loss would thus be to consider a pre-trained convnet, and define as our loss the L2 norm between the \n",
    "activations of a top layer computed over the target image and the activations of the same layer computed over the generated image. This \n",
    "would guarantee that, as seen from the top layer of the convnet, the generated image will \"look similar\" to the original target image. \n",
    "Assuming that what the top layers of a convnet see is really the \"content\" of their input images, then this does work as a way to preserve \n",
    "image content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The style loss\n",
    "\n",
    "\n",
    "While the content loss only leverages a single higher-up layer, the style loss as defined in the Gatys et al. paper leverages multiple \n",
    "layers of a convnet: we aim at capturing the appearance of the style reference image at all spatial scales extracted by the convnet, not \n",
    "just any single scale.\n",
    "\n",
    "For the style loss, the Gatys et al. paper leverages the \"Gram matrix\" of a layer's activations, i.e. the inner product between the feature maps \n",
    "of a given layer. This inner product can be understood as representing a map of the correlations between the features of a layer. These \n",
    "feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically corresponds to the appearance \n",
    "of the textures found at this scale.\n",
    "\n",
    "Hence the style loss aims at preserving similar internal correlations within the activations of different layers, across the style \n",
    "reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales will look similar \n",
    "across the style reference image and the generated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In short\n",
    "\n",
    "\n",
    "In short, we can use a pre-trained convnet to define a loss that will:\n",
    "\n",
    "* Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The \n",
    "convnet should \"see\" both the target image and the generated image as \"containing the same things\".\n",
    "* Preserve style by maintaining similar _correlations_ within activations for both low-level layers and high-level layers. Indeed, feature \n",
    "correlations capture _textures_: the generated and the style reference image should share the same textures at different spatial scales.\n",
    "\n",
    "Now let's take a look at a Keras implementation of the original 2015 neural style transfer algorithm. As you will see, it shares a lot of \n",
    "similarities with the Deep Dream implementation we developed in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural style transfer in Keras\n",
    "\n",
    "\n",
    "Neural style transfer can be implemented using any pre-trained convnet. Here we will use the VGG19 network, used by Gatys et al in their paper. \n",
    "VGG19 is a simple variant of the VGG16 network we introduced in Chapter 5, with three more convolutional layers.\n",
    "\n",
    "This is our general process:\n",
    "\n",
    "* Set up a network that will compute VGG19 layer activations for the style reference image, the target image, and the generated image at \n",
    "the same time.\n",
    "* Use the layer activations computed over these three images to define the loss function described above, which we will minimize in order \n",
    "to achieve style transfer.\n",
    "* Set up a gradient descent process to minimize this loss function.\n",
    "\n",
    "\n",
    "Let's start by defining the paths to the two images we consider: the style reference image and the target image. To make sure that all \n",
    "images processed share similar sizes (widely different sizes would make style transfer more difficult), we will later resize them all to a \n",
    "shared height of 400px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# @gilbellosta, 2022-08-19\n",
    "# Style transfer - basic implementation - using standard Gram matrix\n",
    "# https://github.com/cjgb/style-transfer-beyond-gram/blob/main/python/style_transfer_01_optim.py\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la ruta a las imágenes, una de \"base\" (contenido, \"propia\") y otra de \"estilo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://img-datasets.s3.amazonaws.com/sf.jpg\n",
      "575046/575046 [==============================] - 1s 1us/step\n",
      "Downloading data from https://img-datasets.s3.amazonaws.com/starry_night.jpg\n",
      "943128/943128 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "url_base  = 'https://img-datasets.s3.amazonaws.com/sf.jpg'\n",
    "url_style = 'https://img-datasets.s3.amazonaws.com/starry_night.jpg'\n",
    "\n",
    "base_image_path = keras.utils.get_file('sf.jpg', origin=url_base)\n",
    "style_reference_image_path = keras.utils.get_file('starry_night.jpg', origin=url_style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947   1460\n",
      "533   400\n"
     ]
    }
   ],
   "source": [
    "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height)\n",
    "\n",
    "\n",
    "print(str(original_width) + '   ' + str(original_height))\n",
    "\n",
    "print(str(img_width) + '   ' + str(img_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = keras.utils.load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = keras.utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = keras.applications.vgg19.preprocess_input(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(img):\n",
    "    img = img.reshape((img_height, img_width, 3))\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's set up the VGG19 network. It takes as input a batch of three images: the style reference image, the target image, and a placeholder \n",
    "that will contain the generated image. A placeholder is simply a symbolic tensor, the values of which are provided externally via Numpy \n",
    "arrays. The style reference and target image are static, while the values contained in the placeholder \n",
    "of the generated image will change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We will use VGG19 as the underlying NN.\n",
    "\n",
    "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "\n",
    "content_layer_name = \"block5_conv2\"\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights to calibrate style transfer level\n",
    "\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's define the content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions (as per the original paper)\n",
    "\n",
    "def content_loss(x, y):\n",
    "    return tf.reduce_sum(tf.square(x - y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, here's the style loss. It leverages an auxiliary function to compute the Gram matrix of an input matrix, i.e. a map of the correlations \n",
    "found in the original feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_img, combination_img):\n",
    "    S = style_img\n",
    "    C = gram_matrix(combination_img)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To these two loss components, we add a third one, the \"total variation loss\". It is meant to encourage spatial continuity in the generated \n",
    "image, thus avoiding overly pixelated results. You could interpret it as a regularization loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The loss that we minimize is a weighted average of these three losses. \n",
    "\n",
    "To compute the content loss, we only leverage one top layer, the \n",
    "`block5_conv2` layer, while for the style loss we use a list of layers than spans both low-level and high-level layers. We add the total variation loss at the end.\n",
    "\n",
    "Depending on the style reference image and content image you are using, you will likely want to tune the `content_weight` coefficient, the \n",
    "contribution of the content loss to the total loss. A higher `content_weight` means that the target content will be more recognizable in \n",
    "the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights to calibrate style transfer level\n",
    "\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = tf.square(x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1 , :])\n",
    "    b = tf.square(x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :])\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the base image\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "base_image_features = feature_extractor(base_image)[content_layer_name][0, :, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the style image\n",
    "# Note that the Gram matrix is precalculated\n",
    "\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "style_reference_features = feature_extractor(style_reference_image)\n",
    "style_reference_features = {\n",
    "    layer_id : gram_matrix(style_reference_features[layer_id][0,:,:,])\n",
    "    for layer_id in style_layer_names\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(combination_image, base_image_features, style_reference_features):\n",
    "\n",
    "    loss = tf.zeros(shape=())\n",
    "    features = feature_extractor(combination_image)\n",
    "\n",
    "    # loss associated to the image shape\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, features[content_layer_name][0, :, :, :]\n",
    "    )\n",
    "\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_style_reference_features = style_reference_features[layer_name]\n",
    "        combination_features = features[layer_name][0, :, :, :]\n",
    "\n",
    "        style_loss_value = style_loss(layer_style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
    "\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image_features, style_reference_features):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image_features, style_reference_features)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads\n",
    "\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = 100.0, decay_steps = 100, decay_rate = 0.96\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterations = 5\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image_features, style_reference_features\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "#    if i % 100 == 0:\n",
    "    print(f\"Iteration {i}: loss={loss:.2f}\")\n",
    "    img = deprocess_image(combination_image.numpy())\n",
    "    fname = f\"combination_image_at_iteration_{i}_optimized_code.png\"\n",
    "    keras.utils.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Keep in mind that what this technique achieves is merely a form of image re-texturing, or texture transfer. It will work best with style \n",
    "reference images that are strongly textured and highly self-similar, and with content targets that don't require high levels of details in \n",
    "order to be recognizable. It would typically not be able to achieve fairly abstract feats such as \"transferring the style of one portrait to \n",
    "another\". The algorithm is closer to classical signal processing than to AI, so don't expect it to work like magic!\n",
    "\n",
    "Additionally, do note that running this style transfer algorithm is quite slow. However, the transformation operated by our setup is simple \n",
    "enough that it can be learned by a small, fast feedforward convnet as well -- as long as you have appropriate training data available. Fast \n",
    "style transfer can thus be achieved by first spending a lot of compute cycles to generate input-output training examples for a fixed style \n",
    "reference image, using the above method, and then training a simple convnet to learn this style-specific transformation. Once that is done, \n",
    "stylizing a given image is instantaneous: it's a just a forward pass of this small convnet.\n",
    "\n",
    "\n",
    "## Take aways\n",
    "\n",
    "* Style transfer consists in creating a new image that preserves the \"contents\" of a target image while also capturing the \"style\" of a \n",
    "reference image.\n",
    "* \"Content\" can be captured by the high-level activations of a convnet.\n",
    "* \"Style\" can be captured by the internal correlations of the activations of different layers of a convnet.\n",
    "* Hence deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pre-trained convnet.\n",
    "* Starting from this basic idea, many variants and refinements are possible!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
